{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tidying**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of the data engineering process is data cleaning and tidying. What is done in those two processes, is trying to make the data more readable, and complete. This makes much easier to analyze, visualize, and train the data.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Tidying**\n",
    "\n",
    "Making the data more organized, and readable is the result of applying data tidying. \n",
    "\n",
    "In this section two main pandas functions are used in data tidying those are `melt` and `pivot_table`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by taking a look at the below dataframe, which represents the income ranges based on religion. This is part of the PEW research, which is famous in the US for conducting pollings and surveys on citizens.\n",
    "\n",
    "When the following are satisfied:\n",
    "\n",
    "\n",
    "1. Each variable forms a column\n",
    "2. Each observation forms a row\n",
    "3. Each type of observational unit forms a table\n",
    "\n",
    "We can then say that our dataset is *tidy*.\n",
    "\n",
    "First we need to import pandas to read csv datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # Added seaborn for better visualization\n",
    "from scipy import stats\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PEW Research Dataset**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start by Importing the dataset into a pandas dataframe."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Observe the dataset using the `loc`, `iloc`, `head`, or `tail` approaches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What does not seem right in the above dataframe?***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Try to make the column headers represent a variable not a value. For that, use the `melt` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pew_csvFile = 'data/pew-raw.csv'\n",
    "pew_dataFrame = pd.read_csv(pew_csvFile)\n",
    "\n",
    "#use of loc() function to access a group of rows and columns by labels or a boolean array\n",
    "pew_loc = pew_dataFrame.loc()\n",
    "print(\"\\nUsing loc() function:\")\n",
    "print(pew_loc)\n",
    "\n",
    "#use of iloc() function to access a group of rows and columns by integer position(s)\n",
    "pew_iloc = pew_dataFrame.iloc()\n",
    "print(\"\\nUsing iloc() function:\")\n",
    "print(pew_iloc)\n",
    "\n",
    "#use of head() function to view the first few rows of the DataFrame\n",
    "pew_head = pew_dataFrame.head(4)\n",
    "print(\"\\nUsing head() function:\")\n",
    "print(pew_head)\n",
    "\n",
    "#use of tail() function to view the last few rows of the DataFrame \n",
    "pew_tail = pew_dataFrame.tail(3)\n",
    "print(\"\\nUsing tail() function:\")\n",
    "print(pew_tail)\n",
    "\n",
    "#use of melt() function to transform the DataFrame from wide format to long format\n",
    "# pd.melt(\n",
    "#     frame,               # your DataFrame\n",
    "#     id_vars=None,        # columns to keep fixed (like identifiers)\n",
    "#     value_vars=None,     # columns to unpivot (optional)\n",
    "#     var_name=None,       # name for the new 'variable' column\n",
    "#     value_name='value'   # name for the new 'value' column\n",
    "# )\n",
    "melted = pd.melt(pew_dataFrame,\n",
    "                id_vars='religion',\n",
    "                var_name='income_bracket',\n",
    "                value_name='count')\n",
    "print(\"\\nUsing melt() function:\")\n",
    "print(melted)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Billboard Dataset**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset outlines data about the top hit songs on the Billboard list and the week from entrance that it was in the billboard with the ranking."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the dataset and store it in a pandas dataframe. Note that the usual utf-8 encoding does not work on this dataset. The reason behind this is that there might be characters that are not supported by `utf-8`.\n",
    "\n",
    "The suggestion is to use for this dataset `unicode_escape` encoding. (converts all non-ASCII characters into their \\uXXXX representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "billboard_csvFile = 'data/billboard.csv'\n",
    "# file was enocoded in utf-8 format use encoding='unicode_escape' will help in handle the csv file\n",
    "# pd.read_csv(filepath_or_buffer, encoding=None, ...)\n",
    "billboard_dataFrame = pd.read_csv(billboard_csvFile, encoding='unicode_escape')\n",
    "# above encoding will help in handling special characters in the csv file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Observe the first few rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use of head() function to view the first few rows of the DataFrame\n",
    "print(billboard_dataFrame.head(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What is wrong with the above dataset?***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Let's, again, use the `melt` function to fix the general structure of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all the week columns in Week column\n",
    "# Identify all the week columns\n",
    "# Create a list of all week columns\n",
    "# Use list comprehension to filter columns that start with 'x' and contain 'week'\n",
    "# Example: ['x1.week', 'x2.week', ..., 'x76.week']\n",
    "weekly_cols = [col for col in billboard_dataFrame.columns if col.startswith('x') and 'week' in col]\n",
    "\n",
    "df_melted = pd.melt(billboard_dataFrame,\n",
    "            id_vars=['year', \n",
    "                    'artist.inverted', \n",
    "                    'track', \n",
    "                    'genre', \n",
    "                    'date.entered', \n",
    "                    'date.peaked', \n",
    "                    'time'],\n",
    "            value_vars=weekly_cols,\n",
    "            var_name='week',\n",
    "            value_name='chart_position')\n",
    "\n",
    "print(df_melted.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we inspect the current dataframe. We find that it is structured in a better way than before. \n",
    "\n",
    "However, the ***Week*** column looks a bit ugly!\n",
    "\n",
    "4. Let's try to place only the week number in that column without the extras surronding it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weekly_cols = [col for col in billboard_dataFrame.columns if col.startswith('x') and 'week' in col]\n",
    "\n",
    "# df_melted = pd.melt(billboard_dataFrame,\n",
    "#                     id_vars=['year', 'artist.inverted', 'track', 'genre', 'date.entered', 'date.peaked'],\n",
    "#                     value_vars=weekly_cols,\n",
    "#                     var_name='week',\n",
    "#                     value_name='chart_position')\n",
    "\n",
    "# Extract numeric week number from 'week' column\n",
    "# Convert 'week' column to numeric values by extracting the number from the string\n",
    "# Use str.extract() to get the numeric part and convert to integer\n",
    "# Example: 'x1.week' -> 1, 'x2.week' -> 2, ..., 'x76.week' -> 76\n",
    "# astype(int) converts the extracted string to integer type\n",
    "df_melted['week'] = df_melted['week'].str.extract(r'(\\d+)').astype(int)\n",
    "print(df_melted)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Now let's inspect the ***Week*** column in the dataframe."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's try to find the date at which the song ranked the number that is shown per row.\n",
    "\n",
    "6. To do that let's first think of the equation that is going to get us the relevant date at which the song ranked the *rth*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure date.entered is datetime type\n",
    "# Convert 'date.entered' column to datetime format for proper date handling\n",
    "# Use pd.to_datetime() to convert the column\n",
    "# This ensures that date operations can be performed on this column\n",
    "df_melted['date.entered'] = pd.to_datetime(df_melted['date.entered'])\n",
    "df_melted\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Timedeltas are absolute differences in times, expressed in difference units (e.g. days, hours, minutes, seconds). This method converts an argument from a recognized timedelta format / value into a Timedelta type.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the actual chart date for each entry\n",
    "# Use the 'date.entered' and 'week' columns to calculate the actual date for each chart position\n",
    "# pd.to_timedelta() is used to convert weeks into a timedelta object\n",
    "# The formula adds (week - 1) * 7 days to the 'date.entered' to get the actual chart date\n",
    "# This gives the exact date when the song was at that chart position\n",
    "df_melted['chart_date'] = df_melted['date.entered'] + pd.to_timedelta(df_melted['week'] - 1, unit='D') * 7\n",
    "\n",
    "df_melted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What is the problem with the calculation above?***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Let's only keep necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and reorder relevant columns for final output\n",
    "# Create a new DataFrame with only the relevant columns in a specific order\n",
    "# This makes it easier to analyze and visualize the data\n",
    "# The 'chart_date' column is included to provide context for each chart position\n",
    "# The 'week' column indicates the specific week of the chart\n",
    "# The final DataFrame includes:\n",
    "# 'artist.inverted': Name of the artist\n",
    "# 'track': Name of the track\n",
    "# 'genre': Genre of the track\n",
    "# 'chart_position': Position of the track on the chart\n",
    "# 'week': Week number on the chart\n",
    "# 'chart_date': Actual date corresponding to the chart position\n",
    "df_final = df_melted[[\n",
    "    'artist.inverted',\n",
    "    'track',\n",
    "    'genre',\n",
    "    'chart_position',\n",
    "    'week',\n",
    "    'chart_date'\n",
    "]]\n",
    "\n",
    "df_final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. How to rename your columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for clarity\n",
    "# Rename columns to more user-friendly names\n",
    "# This improves readability and understanding of the DataFrame\n",
    "# The new column names are:\n",
    "# 'artist': Name of the artist\n",
    "# 'title': Name of the track\n",
    "# 'genre': Genre of the track\n",
    "# 'rank': Position of the track on the chart\n",
    "# 'week_number': Week number on the chart\n",
    "# 'date': Actual date corresponding to the chart position\n",
    "# The renaming is done using the rename() method with a dictionary mapping old names to new names\n",
    "df_renamed = df_final.rename(columns={\n",
    "    'artist.inverted': 'artist',\n",
    "    'track': 'title',\n",
    "    'genre': 'genre',\n",
    "    'chart_position': 'rank',\n",
    "    'week': 'week_number',\n",
    "    'chart_date': 'date'\n",
    "})\n",
    "\n",
    "df_renamed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above dataframe, there are some *NaN* values. What are we going to do? <br/>\n",
    "9. Apply quick data cleaning and then observe the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Cleaning**\n",
    "\n",
    "Data cleaning involves removing unwanted characters, imputing, or dropping missing values.\n",
    "\n",
    "The decision is based on the dataset you have, and the information you can extract from the other columns.\n",
    "\n",
    "\n",
    "Examples of data cleaning include cleaning:\n",
    "\n",
    "1.   **Missing Data**\n",
    "2.   **Irregular Data** (Outliers)\n",
    "3.   **Unnecessary Data** — Repetitive Data, Duplicates and more\n",
    "4.   **Inconsistent Data** — Capitalization, Addresses and more\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cars Data Set**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by reading the dataset related to car models: ./CSVs/cars.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to read csv file from /data/cars.csv\n",
    "cars_csvPath = \"data/cars.csv\"\n",
    "#read using and load the file as dataframe as df\n",
    "# cars_dataFrame = pd.read_csv(cars_csvPath, sep=';', na_values=['', '?', 'NA', 'null'])\n",
    "# sep=';' specifies that the delimiter in the CSV file is a semicolon\n",
    "# na_values=['', '?', 'NA', 'null'] specifies additional strings to recognize as NaN\n",
    "# This helps in handling missing or invalid data in the CSV file\n",
    "cars_dataFrame = pd.read_csv(cars_csvPath, sep=';', na_values=['', '?', 'NA', 'null'])\n",
    "cars_dataFrame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Row seems to be the datatype, we need to remove it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first row if it contains metadata or unwanted information\n",
    "# Use iloc to select all rows except the first one (index 0)\n",
    "# reset_index(drop=True) is used to reset the index of the DataFrame after dropping the first row\n",
    "# drop=True ensures that the old index is not added as a new column\n",
    "# This is useful when the first row contains headers or notes that are not part of the actual data\n",
    "cars_dataFrame = cars_dataFrame.iloc[1:].reset_index(drop=True)\n",
    "# OR cars_dataFrame = cars_dataFrame.drop(index=0).reset_index(drop=True)\n",
    "cars_dataFrame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe the columns with null values.  Either by using the `isnull().sum()` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if any cell is missing the data we need to show but after getting 0 as answer, using this code\n",
    "# for making it sure any  '', '?', 'NA', 'null' is also a missing data\n",
    "cars_dataFrame.replace(['', '?', 'NA', 'null'], pd.NA, inplace=True)\n",
    "\n",
    "\n",
    "# Check columns with null values\n",
    "# isnull() checks for missing values in the DataFrame\n",
    "# sum() aggregates the count of missing values for each column\n",
    "# reset_index() converts the Series to a DataFrame for better readability\n",
    "# The resulting DataFrame has two columns: 'Column_Name' and 'Missing_Data'\n",
    "missing_report = cars_dataFrame.isnull().sum().reset_index()\n",
    "missing_report.columns = ['Column_Name', 'Missing_Data']\n",
    "print(missing_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There aren't many missing values. Let's take a glimpse at the percentage of the missing values:\n",
    "\n",
    "**HINT:** We'll need `Numpy` for the below task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentage of missing values for each column\n",
    "# isnull() checks for missing values in the DataFrame\n",
    "# sum() aggregates the count of missing values for each column\n",
    "# Dividing by len(cars_dataFrame) gives the proportion of missing values\n",
    "# Multiplying by 100 converts the proportion to a percentage\n",
    "# np.round(..., 2) rounds the percentage to 2 decimal places for better readability\n",
    "missing_percent = cars_dataFrame.isnull().sum() / len(cars_dataFrame) * 100\n",
    "missing_report = pd.DataFrame({\n",
    "    'column_name': cars_dataFrame.columns,\n",
    "    'missing_percent': np.round(missing_percent.values, 2)\n",
    "})\n",
    "\n",
    "print(missing_report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around *0.19%* of the values are missing, which isn't a lot. Therefore, we might go with the option of dropping all the rows with null values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also check dropping the columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe how many columns we lost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cars Dataset - Filling in missing values automatically**\n",
    "\n",
    "Another option is to try and fill in the missing values through imputations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the `MPG` column for example. We can fill in the missing values with 0s through the following line of code:\n",
    "\n",
    "`df_cars.fillna(0) `. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this does not make much sense as there isn't MPG equal to 0. How about we plot the MPG column and if it follows a random distribution we can use the mean of the column to compute the missing values. Otherwise, we can use the median (if there is a skewed normal distribution). However, there might be a better way of imputation which is getting the median or the mean of the MPG of the cars with similar attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in 'MPG' in cars_dataFrame as 0\n",
    "# Use fillna() to replace NaN values in the 'MPG' column with 0\n",
    "# inplace=True modifies the DataFrame in place without returning a new object\n",
    "# This means that cars_dataFrame will be updated directly\n",
    "cars_dataFrame.fillna({'MPG': 0}, inplace=True)\n",
    "# Print the updated DataFrame to verify that missing values in 'MPG' have been filled with 0\n",
    "print(cars_dataFrame)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we observe the graph above, we can consider it in a way or another normally distributed. Therefore, we can impute the missing values using the mean."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the mean we need numeric values. However the values in the dataframe are objects. Therefore, we need to change them to numerics so that we can compute them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what is the mean of the MPG column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see Mean of MPG\n",
    "# mean() calculates the average of the 'MPG' column, ignoring NaN values\n",
    "mean_mpg = cars_dataFrame['MPG'].mean()\n",
    "print(f\"Mean MPG: {mean_mpg}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this mean to compute the missing values since the graph demonstarted a normal distribution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Car Dataset - Simple Imputer**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*SimpleImputer* is a `scikit-learn` class which is helpful in handling the missing data in the predictive model dataset. It replaces the `NaN` values with a specified placeholder.\n",
    "It is implemented by the use of the `SimpleImputer()` method which takes the following arguments :\n",
    "\n",
    "`missing_values` : The missing_values placeholder which has to be imputed. By default is NaN\n",
    "\n",
    "`strategy` : The data which will replace the NaN values from the dataset. The strategy argument can take the values – ‘mean'(default), ‘median’, ‘most_frequent’ and ‘constant’.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the `SimpleImputer` into our notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need to do are two essential steps:\n",
    "\n",
    "1. fit the data (compute the mean / median / most freq)\n",
    "2. transform the data (place the computed values in the NaN cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Imputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "# Fit the imputer on the DataFrame\n",
    "imputer.fit(cars_dataFrame[['Weight', 'MPG']])\n",
    "# Transform the DataFrame to fill missing values\n",
    "cars_dataFrame[['Weight', 'MPG']] = imputer.transform(cars_dataFrame[['Weight', 'MPG']])\n",
    "# Transform the DataFrame to fill missing values\n",
    "cars_dataFrame[['Weight', 'MPG']] = imputer.transform(cars_dataFrame[['Weight', 'MPG']])\n",
    "cars_dataFrame\n",
    "#!/usr/bin/env python3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Outlier Detection** \n",
    "\n",
    "\n",
    "An Outlier is a data-item/object that deviates significantly from the rest of the (so-called normal)objects. They can be caused by measurement or execution errors. The analysis for outlier detection is referred to as outlier mining. There are many ways to detect the outliers, and the removal process is the data frame same as removing a data item from the panda’s data frame.\n",
    "\n",
    "\n",
    "\n",
    "https://www.geeksforgeeks.org/detect-and-remove-the-outliers-using-python/\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outlier Detection and Removal\n",
    "# from cars_dataFrame we will use Weight and MPG columns for outlier detection\n",
    "# different methods are there for outlier detection\n",
    "# Boxplot method\n",
    "      \n",
    "\n",
    "plt.figure(figsize=(10, 6)) \n",
    "# Using seaborn to create a boxplot\n",
    "sns.boxplot(data=cars_dataFrame[['Weight', 'MPG']])\n",
    "plt.title('Boxplot for Outlier Detection')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Outliers Using Box Plot\n",
    "It captures the summary of the data effectively and efficiently with only a simple box and whiskers. Boxplot summarizes sample data using 25th, 50th, and 75th percentiles. One can just get insights(quartiles, median, and outliers) into the dataset by just looking at its boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot method\n",
    "# Using seaborn to create a boxplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=cars_dataFrame[['Weight', 'MPG']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Outliers Using ScatterPlot.\n",
    "\n",
    "It is used when you have paired numerical data and when your dependent variable has multiple values for each reading independent variable, or when trying to determine the relationship between the two variables. In the process of utilizing the scatter plot, one can also use it for outlier detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Outlier using Scatter Plot\n",
    "    \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(cars_dataFrame.index, cars_dataFrame['Weight'], color='blue', label='Weight')\n",
    "plt.scatter(cars_dataFrame.index, cars_dataFrame['MPG'], color='green', label='MPG')\n",
    "plt.title('Scatter Plot for Outlier Detection')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Z-Score:\n",
    "Z- Score is also called a standard score. This value/score helps to understand that how far is the data point from the mean. And after setting up a threshold value one can utilize z score values of data points to define the outliers.\n",
    "<br>\n",
    "Zscore = (data_point -mean) / std. deviation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to define an outlier threshold value is chosen which is generally 3.0. As 99.7% of the data points lie between +/- 3 standard deviation (using Gaussian Distribution approach).\n",
    "\n",
    "Rows where Z value is greater than 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IQR (Inter-Quartile Range)\n",
    "Inter Quartile Range approach to finding the outliers is the most commonly used and most trusted approach used in the research field. <Br>\n",
    "IQR = Quartile3 - Quartile1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define the outlier base value is defined above and below dataset’s normal range namely Upper and Lower bounds, define the upper and the lower bound (1.5*IQR value is considered) :<br>\n",
    "upper = Q3 + 1.5 * IQR <br>\n",
    "lower = Q1 - 1.5 * IQR <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Removing the outliers:\n",
    "For removing the outlier, one must follow the same process of removing an entry from the dataset using its exact position in the dataset because in all the above methods of detecting the outliers end result is the list of all those data items that satisfy the outlier definition according to the method used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
